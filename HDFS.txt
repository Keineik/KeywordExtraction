HDFS được thiết kế và tối ưu hóa để lưu trữ các tập tin rất lớn và với một mô hình truy cập trực tuyến. Kể từ khi nó được dự kiến sẽ chạy trên phần cứng thương mại, nó được thiết kế để tính đến và xử lý sự cố trên máy cá nhân. HDFS thông thường không lưu trữ dữ liệu chính. Thay vào đó, trong một việc điển hình, dữ liệu được sao chép sang HDFS cho mục đích thực hiện MapReduce, và kết quả sau đó sẽ được sao chép sang HDFS. Từ khi HDFS được tối ưu hóa cho các luồng truy cập thông tin lớn, truy cập ngẫu nhiên vào các thành phần của các tập tin thì tốn kém hơn truy cập tuần tự rõ ràng, và nó cũng không hỗ trợ cập nhật các tập tin, chỉ có thể gắn thêm. Các kịch bản điển hình của các ứng dụng sử dụng HDFS sau mô hình truy cập ghi một lần đọc nhiều lần
Các tập tin trong HDFS được chia thành một số khối lớn (thường là 64Mb) và được lưu trong Data Node. Một tập tin điển hình được phân phối trên một số DataNodes nhằm thúc đẩy băng thông cao và xử lý song song. Để nâng cao độ tin cậy, các khối dữ liệu trong HDFS (data blocks) được nhân bản và lưu trỡ trong 3 máy, với một trong những bản sao trong mỗi giá khác nhau để tăng sự sẵn có hơn nữa của dữ liệu. Việc duy trì các tập tin siêu dữ liệu được xử lý bởi một NameNode riêng biệt. Siêu dữ liệu này bao gồm ánh xạ từ tập tin vào khối và vị trí của khối (DataNode). NameNode theo định kì sẽ truyền đạt siêu dữ liệu của nó cho Secondary NameNode khi cần cấu hình lại để làm nhiệm của NameNode khi xảy ra trường hợp lỗi.